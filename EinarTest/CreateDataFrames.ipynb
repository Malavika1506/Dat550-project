{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import functions as fun\n",
    "from collections import Counter\n",
    "import ast\n",
    "import pickle\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import pydot\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint \n",
    "import pandas as pd\n",
    "import scipy as sp \n",
    "import string\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import sklearn.model_selection as ms\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import tensorboard as ts\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "#tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claimID</th>\n",
       "      <th>claim</th>\n",
       "      <th>label</th>\n",
       "      <th>claimURL</th>\n",
       "      <th>reason</th>\n",
       "      <th>categories</th>\n",
       "      <th>speaker</th>\n",
       "      <th>checker</th>\n",
       "      <th>tags</th>\n",
       "      <th>articleTitle</th>\n",
       "      <th>publishDate</th>\n",
       "      <th>claimDate</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pomt-03627</td>\n",
       "      <td>Six out of 10 of the highest unemployment rate...</td>\n",
       "      <td>half-true</td>\n",
       "      <td>/ohio/statements/2013/may/06/chris-redfern/ohi...</td>\n",
       "      <td>When a couple of Statehouse Republicans prepar...</td>\n",
       "      <td>None</td>\n",
       "      <td>Chris Redfern</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2013-05-06T06:00:00</td>\n",
       "      <td>2013-04-30</td>\n",
       "      <td>['None']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pomt-09611</td>\n",
       "      <td>No Democratic campaign for (Fla.) governor has...</td>\n",
       "      <td>true</td>\n",
       "      <td>/florida/statements/2010/jan/15/alex-sink/flor...</td>\n",
       "      <td>Florida's leading Republican candidate for gov...</td>\n",
       "      <td>None</td>\n",
       "      <td>Alex Sink</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2010-01-15T13:59:00</td>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>['None']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tron-00214</td>\n",
       "      <td>Forward an email for Jasmine</td>\n",
       "      <td>fiction!</td>\n",
       "      <td>https://www.truthorfiction.com/jasmine/</td>\n",
       "      <td>None</td>\n",
       "      <td>9-11-attack</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Forward an email for Jasmine</td>\n",
       "      <td>Mar 17, 2015</td>\n",
       "      <td>None</td>\n",
       "      <td>['None']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>snes-04484</td>\n",
       "      <td>Pope Francis endorsed Donald Trump for president.</td>\n",
       "      <td>false</td>\n",
       "      <td>https://www.snopes.com/fact-check/pope-francis...</td>\n",
       "      <td>None</td>\n",
       "      <td>Junk News</td>\n",
       "      <td>None</td>\n",
       "      <td>Dan Evon</td>\n",
       "      <td>None</td>\n",
       "      <td>Pope Francis Shocks World, Endorses Donald Tru...</td>\n",
       "      <td>10 July 2016</td>\n",
       "      <td>None</td>\n",
       "      <td>['None']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pomt-06704</td>\n",
       "      <td>Says Ron Paul insisted FEMA should be shut down.</td>\n",
       "      <td>true</td>\n",
       "      <td>/texas/statements/2011/sep/03/maureen-dowd/mau...</td>\n",
       "      <td>Commenting on the federal response to Hurrican...</td>\n",
       "      <td>None</td>\n",
       "      <td>Maureen Dowd</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2011-09-03T06:00:00</td>\n",
       "      <td>2011-08-30</td>\n",
       "      <td>['None']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      claimID                                              claim      label  \\\n",
       "0  pomt-03627  Six out of 10 of the highest unemployment rate...  half-true   \n",
       "1  pomt-09611  No Democratic campaign for (Fla.) governor has...       true   \n",
       "2  tron-00214                       Forward an email for Jasmine   fiction!   \n",
       "3  snes-04484  Pope Francis endorsed Donald Trump for president.      false   \n",
       "4  pomt-06704   Says Ron Paul insisted FEMA should be shut down.       true   \n",
       "\n",
       "                                            claimURL  \\\n",
       "0  /ohio/statements/2013/may/06/chris-redfern/ohi...   \n",
       "1  /florida/statements/2010/jan/15/alex-sink/flor...   \n",
       "2            https://www.truthorfiction.com/jasmine/   \n",
       "3  https://www.snopes.com/fact-check/pope-francis...   \n",
       "4  /texas/statements/2011/sep/03/maureen-dowd/mau...   \n",
       "\n",
       "                                              reason   categories  \\\n",
       "0  When a couple of Statehouse Republicans prepar...         None   \n",
       "1  Florida's leading Republican candidate for gov...         None   \n",
       "2                                               None  9-11-attack   \n",
       "3                                               None    Junk News   \n",
       "4  Commenting on the federal response to Hurrican...         None   \n",
       "\n",
       "         speaker   checker  tags  \\\n",
       "0  Chris Redfern      None  None   \n",
       "1      Alex Sink      None  None   \n",
       "2           None      None  None   \n",
       "3           None  Dan Evon  None   \n",
       "4   Maureen Dowd      None  None   \n",
       "\n",
       "                                        articleTitle          publishDate  \\\n",
       "0                                               None  2013-05-06T06:00:00   \n",
       "1                                               None  2010-01-15T13:59:00   \n",
       "2                       Forward an email for Jasmine         Mar 17, 2015   \n",
       "3  Pope Francis Shocks World, Endorses Donald Tru...         10 July 2016   \n",
       "4                                               None  2011-09-03T06:00:00   \n",
       "\n",
       "    claimDate  entities  \n",
       "0  2013-04-30  ['None']  \n",
       "1  2010-01-06  ['None']  \n",
       "2        None  ['None']  \n",
       "3        None  ['None']  \n",
       "4  2011-08-30  ['None']  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = ['claimID', 'claim', 'label', 'claimURL', 'reason', 'categories', 'speaker', 'checker', 'tags', 'articleTitle', 'publishDate', 'claimDate', 'entities']\n",
    "df_train = pd.read_csv('../data/train.tsv',sep='\\t',names=h,header=None)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,     3,     4, ..., 23997, 23999, 24002], dtype=int64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_train.reset_index(drop=False, inplace=True)\n",
    "indexw = df_train.index[(df_train['label']=='true') | (df_train['label']=='false')].to_numpy()\n",
    "\n",
    "indexw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>snip_1</th>\n",
       "      <th>snip_2</th>\n",
       "      <th>snip_url</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claimID</th>\n",
       "      <th>snip_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">abbc-00001</th>\n",
       "      <th>2</th>\n",
       "      <td>Record numbers march down Brisbane city to pro...</td>\n",
       "      <td>Sep 4, 2018 ... THOUSANDS of people joined the...</td>\n",
       "      <td>http://catholicleader.com.au/news/record-numbe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fact check: Will Queensland's proposed abortio...</td>\n",
       "      <td>Oct 3, 2018 ... Protestors have marched throug...</td>\n",
       "      <td>http://abortion-news.info/fact-check-will-quee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Queensland Council for Civil Liberties - Wikip...</td>\n",
       "      <td>The Queensland Council for Civil Liberties (QC...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Queensland_Counc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Abortion law change expected from Queensland L...</td>\n",
       "      <td>Jun 28, 2018 ... The Queensland Law Reform Com...</td>\n",
       "      <td>http://www.abc.net.au/news/2018-06-29/abortion...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Struggle</td>\n",
       "      <td>enforcing the State‟s controversial anti-march...</td>\n",
       "      <td>https://www.childrenbychoice.org.au/images/dow...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               snip_1  \\\n",
       "claimID    snip_id                                                      \n",
       "abbc-00001 2        Record numbers march down Brisbane city to pro...   \n",
       "           3        Fact check: Will Queensland's proposed abortio...   \n",
       "           4        Queensland Council for Civil Liberties - Wikip...   \n",
       "           5        Abortion law change expected from Queensland L...   \n",
       "           6                                             The Struggle   \n",
       "\n",
       "                                                               snip_2  \\\n",
       "claimID    snip_id                                                      \n",
       "abbc-00001 2        Sep 4, 2018 ... THOUSANDS of people joined the...   \n",
       "           3        Oct 3, 2018 ... Protestors have marched throug...   \n",
       "           4        The Queensland Council for Civil Liberties (QC...   \n",
       "           5        Jun 28, 2018 ... The Queensland Law Reform Com...   \n",
       "           6        enforcing the State‟s controversial anti-march...   \n",
       "\n",
       "                                                             snip_url  \n",
       "claimID    snip_id                                                     \n",
       "abbc-00001 2        http://catholicleader.com.au/news/record-numbe...  \n",
       "           3        http://abortion-news.info/fact-check-will-quee...  \n",
       "           4        https://en.wikipedia.org/wiki/Queensland_Counc...  \n",
       "           5        http://www.abc.net.au/news/2018-06-29/abortion...  \n",
       "           6        https://www.childrenbychoice.org.au/images/dow...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = ['snip_id', 'snip_1', 'snip_2','snip_url']\n",
    "df_snip =  pd.DataFrame(columns=h)\n",
    "\n",
    "\n",
    "dir = '..\\data\\snippets'\n",
    "for filename in os.listdir(dir):\n",
    "    \n",
    "    df_row = pd.read_csv(os.path.join(dir, filename),names=h,sep='\\t',header=None,engine='python',encoding='utf8' ,quoting=3)\n",
    "    df_row.insert(loc = 0, column = 'claimID', value = filename )\n",
    "    \n",
    "    df_snip = df_snip.append(df_row)\n",
    "\n",
    "df_snip.set_index(['claimID','snip_id'],inplace = False).head()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save training df to pickle.  (Remove rows with missing snippets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (24004, 13)\n",
      "Snip shape: (219021, 5) Unique: 24004\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_snip = df_snip.loc[df_snip['claimID'].isin(df_train.claimID)]\n",
    "df_train = df_train.loc[df_train['claimID'].isin(df_snip['claimID'].unique())]\n",
    "\n",
    "\n",
    "print('train shape:',np.shape(df_train))\n",
    "print('Snip shape:',np.shape(df_snip),'Unique:',len(df_snip['claimID'].unique()))\n",
    "\n",
    "df_train.set_index('claimID',inplace = True)\n",
    "df_snip.set_index(['claimID','snip_id'],inplace = True)\n",
    "\n",
    "\n",
    "df_snip.to_pickle(\"./df_snip.pkl\")\n",
    "df_train.to_pickle(\"./df_train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snip = pd.read_pickle(\"./df_snip.pkl\")\n",
    "#df_train = pd.read_pickle(\"./df_train.pkl\")\n",
    "\n",
    "#df_train.reset_index(drop=False, inplace=True)\n",
    "#df_snip.reset_index(drop=False, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_num =  df_train['label'].astype(\"category\").cat.codes.values\n",
    "\n",
    "with open('y_train_num','wb') as i:\n",
    "       pickle.dump(y_train_num, i) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = df_train[['claimURL','speaker','checker']].copy() \n",
    "\n",
    "\n",
    "#df_meta['articleTitle'] = df_meta['articleTitle'].str.lower()\n",
    "\n",
    "df_meta['claimURL'] = df_meta.apply(lambda x: fun.clean_url(x.claimURL),axis=1)\n",
    "\n",
    "df_meta['claimURL'] = df_meta['claimURL'].astype(\"category\").cat.codes.values\n",
    "\n",
    "df_meta['speaker'] = df_meta['speaker'].astype(\"category\").cat.codes.values\n",
    "df_meta['checker'] = df_meta['checker'].astype(\"category\").cat.codes.values\n",
    "\n",
    "    \n",
    "df_meta.head() \n",
    "\n",
    "X_metadata = df_meta.to_numpy()\n",
    "\n",
    "bin_enteties = fun.create_binary(df_train['entities'],30)\n",
    "\n",
    "tags = df_train.apply(lambda x: str(x.tags).replace('[','').replace(']','').replace(\"'\",'').split(',')   ,axis=1)\n",
    "\n",
    "bin_tags = fun.create_binary(tags,30)\n",
    "\n",
    "\n",
    "\n",
    "X_metadata = np.append(X_metadata, bin_enteties, axis=1)\n",
    "\n",
    "with open('X_meta','wb') as i:\n",
    "       pickle.dump(X_metadata, i)\n",
    "\n",
    "\n",
    "df_meta_num = pd.DataFrame(data = np.hstack((df_train.index.to_numpy().reshape(-1,1),X_metadata))     )        \n",
    "        \n",
    "with open('df_meta_num','wb') as i:\n",
    "       pickle.dump(df_meta_num, i)        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claim set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_claim = df_train['claim'].str.lower().copy()\n",
    "\n",
    "words_list = [i if type(i)==str else '' for i in df_claim]\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(words_list)\n",
    "\n",
    "sequences = t.texts_to_sequences(words_list)\n",
    "\n",
    "X_claim = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=50)\n",
    "\n",
    "\n",
    "with open('X_claim','wb') as i:\n",
    "       pickle.dump(X_claim, i)\n",
    "            \n",
    "\n",
    "            \n",
    "df_claim_num = pd.DataFrame(data = np.hstack((df_train.index.to_numpy().reshape(-1,1),X_claim))   )        \n",
    "        \n",
    "with open('df_claim_num','wb') as i:\n",
    "       pickle.dump(df_claim_num, i) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snippet set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snip['snip_url'] = df_snip.apply(lambda x: fun.clean_url(x.snip_url),axis=1)\n",
    "\n",
    "df_snip['snip_1'] = df_snip['snip_1'].str.lower()\n",
    "df_snip['snip_2'] = df_snip['snip_2'].str.lower()\n",
    "\n",
    "df_snip['snip_url'] = df_snip['snip_url'].astype(\"category\").cat.codes.values\n",
    "\n",
    "\n",
    "df_snip.reset_index(drop=False, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words_list1 = [i if type(i)==str else '' for i in df_snip['snip_1']]\n",
    "sequences1 = t.texts_to_sequences(words_list1)\n",
    "snip1 = tf.keras.preprocessing.sequence.pad_sequences(sequences1, maxlen=20)\n",
    "\n",
    "words_list2 = [i if type(i)==str else '' for i in df_snip['snip_2']]\n",
    "sequences2 = t.texts_to_sequences(words_list2)\n",
    "snip2 = tf.keras.preprocessing.sequence.pad_sequences(sequences2, maxlen=80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "snip_info = df_snip[['claimID','snip_id','snip_url']].to_numpy()\n",
    "snip = np.append(snip_info, np.append(snip1,snip2,axis=1), axis=1)\n",
    "\n",
    "\n",
    "df_snip_num =pd.DataFrame(data=snip)\n",
    "df_snip_num\n",
    "\n",
    "with open('df_snip_num','wb') as i:\n",
    "       pickle.dump(df_snip_num, i) \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abbc-00002</td>\n",
       "      <td>2</td>\n",
       "      <td>20830</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>776</td>\n",
       "      <td>100</td>\n",
       "      <td>8</td>\n",
       "      <td>93</td>\n",
       "      <td>16</td>\n",
       "      <td>186</td>\n",
       "      <td>5</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abbc-00002</td>\n",
       "      <td>3</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>59</td>\n",
       "      <td>580</td>\n",
       "      <td>1</td>\n",
       "      <td>3849</td>\n",
       "      <td>681</td>\n",
       "      <td>5</td>\n",
       "      <td>2501</td>\n",
       "      <td>2372</td>\n",
       "      <td>12</td>\n",
       "      <td>13286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abbc-00002</td>\n",
       "      <td>4</td>\n",
       "      <td>7987</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>69</td>\n",
       "      <td>5</td>\n",
       "      <td>448</td>\n",
       "      <td>624</td>\n",
       "      <td>17744</td>\n",
       "      <td>12</td>\n",
       "      <td>1724</td>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abbc-00002</td>\n",
       "      <td>5</td>\n",
       "      <td>8442</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>349</td>\n",
       "      <td>626</td>\n",
       "      <td>136</td>\n",
       "      <td>34</td>\n",
       "      <td>166</td>\n",
       "      <td>6</td>\n",
       "      <td>589</td>\n",
       "      <td>10</td>\n",
       "      <td>1188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abbc-00002</td>\n",
       "      <td>6</td>\n",
       "      <td>21017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>5</td>\n",
       "      <td>6907</td>\n",
       "      <td>7814</td>\n",
       "      <td>714</td>\n",
       "      <td>1</td>\n",
       "      <td>1953</td>\n",
       "      <td>5018</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0   1      2   3   4   5   6   7   8   9    ... 93   94   95    96   \\\n",
       "0  abbc-00002   2  20830   0   0   0   0   0   0   0  ...   5    1  776   100   \n",
       "1  abbc-00002   3    175   0   0   0   0   0   0   0  ...  59  580    1  3849   \n",
       "2  abbc-00002   4   7987   0   0   0   0   0   0   0  ...  69    5  448   624   \n",
       "3  abbc-00002   5   8442   0   0   0   0   0   0   0  ...   5  349  626   136   \n",
       "4  abbc-00002   6  21017   0   0   0   0   0   0   0  ...   1  165    5  6907   \n",
       "\n",
       "     97   98    99    100   101    102  \n",
       "0      8   93    16   186     5    452  \n",
       "1    681    5  2501  2372    12  13286  \n",
       "2  17744   12  1724     1   256    493  \n",
       "3     34  166     6   589    10   1188  \n",
       "4   7814  714     1  1953  5018      2  \n",
       "\n",
       "[5 rows x 103 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_snip_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flat Snippet set (very very slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-234-50dfdc2c35f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf_snip_num\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mdf_snip_num\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\ops\\common.py\u001b[0m in \u001b[0;36mnew_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[0mrvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m         \u001b[0mres_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomparison_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_construct_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m         \u001b[0mres_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomp_method_OBJECT_ARRAY\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[1;34m(op, x, y)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscalar_compare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "snippet_list = []\n",
    "\n",
    "for q,i in enumerate(df_train.index):\n",
    "\n",
    "    row = []\n",
    "    for i,r in df_snip_num[ df_snip_num[0]==i].iterrows():\n",
    "       \n",
    "        row.append(r[1:].to_list())\n",
    "    row = [snip for snippets in row for snip in snippets]\n",
    "    \n",
    "    snippet_list.append(row)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_snip_flat =  tf.keras.preprocessing.sequence.pad_sequences(snippet_list, maxlen=1005)\n",
    "\n",
    "with open('X_snip_flat','wb') as i:\n",
    "       pickle.dump(X_snip_flat, i) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claim (24004, 50)\n",
      "meta (24004, 33)\n",
      "snip flat (24004, 1005)\n",
      "y (24004,)\n"
     ]
    }
   ],
   "source": [
    "print('claim',np.shape(X_claim))\n",
    "print('meta',np.shape(X_metadata))\n",
    "print('snip flat',np.shape(X_snip_flat))\n",
    "print('y',np.shape(y_train_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,  511,  127, ...,    0,    0,    0],\n",
       "       [   0,   46,  127, ...,    0,    0,    0],\n",
       "       [  35, 2243,  127, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   0, 2181,  127, ...,    0,    0,    0],\n",
       "       [   0, 1790,  127, ...,    0,    0,    0],\n",
       "       [   0,  289,  127, ...,    0,    0,    0]], dtype=int16)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([96, 43, 96, ..., 43, 43, 43], dtype=int8)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_num[indexw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>claimID</th>\n",
       "      <th>claim</th>\n",
       "      <th>label</th>\n",
       "      <th>claimURL</th>\n",
       "      <th>reason</th>\n",
       "      <th>categories</th>\n",
       "      <th>speaker</th>\n",
       "      <th>checker</th>\n",
       "      <th>tags</th>\n",
       "      <th>articleTitle</th>\n",
       "      <th>publishDate</th>\n",
       "      <th>claimDate</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>pomt-09611</td>\n",
       "      <td>No Democratic campaign for (Fla.) governor has...</td>\n",
       "      <td>true</td>\n",
       "      <td>/florida/statements/2010/jan/15/alex-sink/flor...</td>\n",
       "      <td>Florida's leading Republican candidate for gov...</td>\n",
       "      <td>None</td>\n",
       "      <td>Alex Sink</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2010-01-15T13:59:00</td>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>['None']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>snes-04484</td>\n",
       "      <td>Pope Francis endorsed Donald Trump for president.</td>\n",
       "      <td>false</td>\n",
       "      <td>https://www.snopes.com/fact-check/pope-francis...</td>\n",
       "      <td>None</td>\n",
       "      <td>Junk News</td>\n",
       "      <td>None</td>\n",
       "      <td>Dan Evon</td>\n",
       "      <td>None</td>\n",
       "      <td>Pope Francis Shocks World, Endorses Donald Tru...</td>\n",
       "      <td>10 July 2016</td>\n",
       "      <td>None</td>\n",
       "      <td>['None']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>pomt-06704</td>\n",
       "      <td>Says Ron Paul insisted FEMA should be shut down.</td>\n",
       "      <td>true</td>\n",
       "      <td>/texas/statements/2011/sep/03/maureen-dowd/mau...</td>\n",
       "      <td>Commenting on the federal response to Hurrican...</td>\n",
       "      <td>None</td>\n",
       "      <td>Maureen Dowd</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2011-09-03T06:00:00</td>\n",
       "      <td>2011-08-30</td>\n",
       "      <td>['None']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>pomt-00986</td>\n",
       "      <td>The 2012 National Survey on Drug Use and Healt...</td>\n",
       "      <td>true</td>\n",
       "      <td>/rhode-island/statements/2015/feb/09/susan-sha...</td>\n",
       "      <td>Vigorous debate surrounds efforts to legalize ...</td>\n",
       "      <td>None</td>\n",
       "      <td>Susan Shapiro</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-02-09T00:01:00</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>['United_States']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>pomt-11794</td>\n",
       "      <td>Says Doug Jones \"is for full-term abortion.\"</td>\n",
       "      <td>false</td>\n",
       "      <td>/truth-o-meter/statements/2017/nov/21/kayla-mo...</td>\n",
       "      <td>As Alabama Republican Senate nominee Roy Moore...</td>\n",
       "      <td>None</td>\n",
       "      <td>Kayla Moore</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-11-21T13:53:21</td>\n",
       "      <td>2017-11-17</td>\n",
       "      <td>['None']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23993</th>\n",
       "      <td>23993</td>\n",
       "      <td>23993</td>\n",
       "      <td>pomt-07162</td>\n",
       "      <td>The average person \"will pay $6,000 more a yea...</td>\n",
       "      <td>true</td>\n",
       "      <td>/ohio/statements/2011/jun/13/tim-ryan/rep-tim-...</td>\n",
       "      <td>Rep. Tim Ryan, a Democrat from Niles, Ohio, sh...</td>\n",
       "      <td>None</td>\n",
       "      <td>Tim Ryan</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2011-06-13T12:35:28</td>\n",
       "      <td>2011-05-24</td>\n",
       "      <td>['Paul_Ryan', 'Medicare_(United_States)']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23994</th>\n",
       "      <td>23994</td>\n",
       "      <td>23994</td>\n",
       "      <td>snes-03990</td>\n",
       "      <td>Jennifer Aniston went on an angry tirade on th...</td>\n",
       "      <td>false</td>\n",
       "      <td>https://www.snopes.com/fact-check/jennifer-ani...</td>\n",
       "      <td>None</td>\n",
       "      <td>Junk News</td>\n",
       "      <td>None</td>\n",
       "      <td>Dan Evon</td>\n",
       "      <td>None</td>\n",
       "      <td>Jennifer Aniston Delivers Angry Response to ‘B...</td>\n",
       "      <td>20 September 2016</td>\n",
       "      <td>None</td>\n",
       "      <td>['Brad_Pitt', 'Jennifer_Aniston', 'Angelina_Jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23997</th>\n",
       "      <td>23997</td>\n",
       "      <td>23997</td>\n",
       "      <td>para-00050</td>\n",
       "      <td>When Abbott was Health Minister there was a sh...</td>\n",
       "      <td>false</td>\n",
       "      <td>http://pandora.nla.gov.au//pan/140601/20131209...</td>\n",
       "      <td>None</td>\n",
       "      <td>['Health', 'Workforce Planning']</td>\n",
       "      <td>Tanya Plibersek</td>\n",
       "      <td>Flynn Murphy, Peter Fray</td>\n",
       "      <td>None</td>\n",
       "      <td>Did Tony Abbott create a GP shortage?</td>\n",
       "      <td>Tuesday, August 27, 2013 at 10:27 a.m.</td>\n",
       "      <td>None</td>\n",
       "      <td>['General_practitioner']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23999</th>\n",
       "      <td>23999</td>\n",
       "      <td>23999</td>\n",
       "      <td>snes-00146</td>\n",
       "      <td>Did Google Fail to Promote President Trump’s S...</td>\n",
       "      <td>false</td>\n",
       "      <td>https://www.snopes.com/fact-check/google-sotu-...</td>\n",
       "      <td>None</td>\n",
       "      <td>Politics</td>\n",
       "      <td>None</td>\n",
       "      <td>Dan Evon</td>\n",
       "      <td>None</td>\n",
       "      <td>Did Google Fail to Promote President Trump’s S...</td>\n",
       "      <td>30 August 2018</td>\n",
       "      <td>None</td>\n",
       "      <td>['None']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24002</th>\n",
       "      <td>24002</td>\n",
       "      <td>24002</td>\n",
       "      <td>pomt-10920</td>\n",
       "      <td>Even the CBO numbers show now that the entire ...</td>\n",
       "      <td>false</td>\n",
       "      <td>/truth-o-meter/statements/2018/jul/31/larry-ku...</td>\n",
       "      <td>With the economy growing at 4.1 percent in the...</td>\n",
       "      <td>None</td>\n",
       "      <td>Larry Kudlow</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-07-31T07:00:00</td>\n",
       "      <td>2018-07-29</td>\n",
       "      <td>['Congressional_Budget_Office']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6728 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       level_0  index     claimID  \\\n",
       "1            1      1  pomt-09611   \n",
       "3            3      3  snes-04484   \n",
       "4            4      4  pomt-06704   \n",
       "8            8      8  pomt-00986   \n",
       "24          24     24  pomt-11794   \n",
       "...        ...    ...         ...   \n",
       "23993    23993  23993  pomt-07162   \n",
       "23994    23994  23994  snes-03990   \n",
       "23997    23997  23997  para-00050   \n",
       "23999    23999  23999  snes-00146   \n",
       "24002    24002  24002  pomt-10920   \n",
       "\n",
       "                                                   claim  label  \\\n",
       "1      No Democratic campaign for (Fla.) governor has...   true   \n",
       "3      Pope Francis endorsed Donald Trump for president.  false   \n",
       "4       Says Ron Paul insisted FEMA should be shut down.   true   \n",
       "8      The 2012 National Survey on Drug Use and Healt...   true   \n",
       "24          Says Doug Jones \"is for full-term abortion.\"  false   \n",
       "...                                                  ...    ...   \n",
       "23993  The average person \"will pay $6,000 more a yea...   true   \n",
       "23994  Jennifer Aniston went on an angry tirade on th...  false   \n",
       "23997  When Abbott was Health Minister there was a sh...  false   \n",
       "23999  Did Google Fail to Promote President Trump’s S...  false   \n",
       "24002  Even the CBO numbers show now that the entire ...  false   \n",
       "\n",
       "                                                claimURL  \\\n",
       "1      /florida/statements/2010/jan/15/alex-sink/flor...   \n",
       "3      https://www.snopes.com/fact-check/pope-francis...   \n",
       "4      /texas/statements/2011/sep/03/maureen-dowd/mau...   \n",
       "8      /rhode-island/statements/2015/feb/09/susan-sha...   \n",
       "24     /truth-o-meter/statements/2017/nov/21/kayla-mo...   \n",
       "...                                                  ...   \n",
       "23993  /ohio/statements/2011/jun/13/tim-ryan/rep-tim-...   \n",
       "23994  https://www.snopes.com/fact-check/jennifer-ani...   \n",
       "23997  http://pandora.nla.gov.au//pan/140601/20131209...   \n",
       "23999  https://www.snopes.com/fact-check/google-sotu-...   \n",
       "24002  /truth-o-meter/statements/2018/jul/31/larry-ku...   \n",
       "\n",
       "                                                  reason  \\\n",
       "1      Florida's leading Republican candidate for gov...   \n",
       "3                                                   None   \n",
       "4      Commenting on the federal response to Hurrican...   \n",
       "8      Vigorous debate surrounds efforts to legalize ...   \n",
       "24     As Alabama Republican Senate nominee Roy Moore...   \n",
       "...                                                  ...   \n",
       "23993  Rep. Tim Ryan, a Democrat from Niles, Ohio, sh...   \n",
       "23994                                               None   \n",
       "23997                                               None   \n",
       "23999                                               None   \n",
       "24002  With the economy growing at 4.1 percent in the...   \n",
       "\n",
       "                             categories          speaker  \\\n",
       "1                                  None        Alex Sink   \n",
       "3                             Junk News             None   \n",
       "4                                  None     Maureen Dowd   \n",
       "8                                  None    Susan Shapiro   \n",
       "24                                 None      Kayla Moore   \n",
       "...                                 ...              ...   \n",
       "23993                              None         Tim Ryan   \n",
       "23994                         Junk News             None   \n",
       "23997  ['Health', 'Workforce Planning']  Tanya Plibersek   \n",
       "23999                          Politics             None   \n",
       "24002                              None     Larry Kudlow   \n",
       "\n",
       "                        checker  tags  \\\n",
       "1                          None  None   \n",
       "3                      Dan Evon  None   \n",
       "4                          None  None   \n",
       "8                          None  None   \n",
       "24                         None  None   \n",
       "...                         ...   ...   \n",
       "23993                      None  None   \n",
       "23994                  Dan Evon  None   \n",
       "23997  Flynn Murphy, Peter Fray  None   \n",
       "23999                  Dan Evon  None   \n",
       "24002                      None  None   \n",
       "\n",
       "                                            articleTitle  \\\n",
       "1                                                   None   \n",
       "3      Pope Francis Shocks World, Endorses Donald Tru...   \n",
       "4                                                   None   \n",
       "8                                                   None   \n",
       "24                                                  None   \n",
       "...                                                  ...   \n",
       "23993                                               None   \n",
       "23994  Jennifer Aniston Delivers Angry Response to ‘B...   \n",
       "23997              Did Tony Abbott create a GP shortage?   \n",
       "23999  Did Google Fail to Promote President Trump’s S...   \n",
       "24002                                               None   \n",
       "\n",
       "                                  publishDate   claimDate  \\\n",
       "1                         2010-01-15T13:59:00  2010-01-06   \n",
       "3                                10 July 2016        None   \n",
       "4                         2011-09-03T06:00:00  2011-08-30   \n",
       "8                         2015-02-09T00:01:00  2015-01-15   \n",
       "24                        2017-11-21T13:53:21  2017-11-17   \n",
       "...                                       ...         ...   \n",
       "23993                     2011-06-13T12:35:28  2011-05-24   \n",
       "23994                       20 September 2016        None   \n",
       "23997  Tuesday, August 27, 2013 at 10:27 a.m.        None   \n",
       "23999                          30 August 2018        None   \n",
       "24002                     2018-07-31T07:00:00  2018-07-29   \n",
       "\n",
       "                                                entities  \n",
       "1                                               ['None']  \n",
       "3                                               ['None']  \n",
       "4                                               ['None']  \n",
       "8                                      ['United_States']  \n",
       "24                                              ['None']  \n",
       "...                                                  ...  \n",
       "23993          ['Paul_Ryan', 'Medicare_(United_States)']  \n",
       "23994  ['Brad_Pitt', 'Jennifer_Aniston', 'Angelina_Jo...  \n",
       "23997                           ['General_practitioner']  \n",
       "23999                                           ['None']  \n",
       "24002                    ['Congressional_Budget_Office']  \n",
       "\n",
       "[6728 rows x 15 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.loc[indexw]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,  511,  127],\n",
       "       [   0,   46,  127],\n",
       "       [  35, 2243,  127],\n",
       "       ...,\n",
       "       [   0, 2181,  127],\n",
       "       [   0, 1790,  127],\n",
       "       [   0,  289,  127]], dtype=int16)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_metadata[:,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = ms.train_test_split(X_metadata, y_train_num, train_size=0.65,test_size=0.35, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 33, 128)           6400000   \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 6,531,713\n",
      "Trainable params: 6,531,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 15602 input samples and 4373 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-134-01cfb74b2225>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rmsprop'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    594\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[1;32m--> 646\u001b[1;33m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[0;32m    647\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2383\u001b[1;33m         batch_size=batch_size)\n\u001b[0m\u001b[0;32m   2384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2483\u001b[0m       \u001b[1;31m# Check that all arrays have the same length.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2484\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2485\u001b[1;33m         \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_array_lengths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2486\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mrun_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2487\u001b[0m           \u001b[1;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_array_lengths\u001b[1;34m(inputs, targets, weights)\u001b[0m\n\u001b[0;32m    742\u001b[0m                      \u001b[1;34m'the same number of samples as target arrays. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    743\u001b[0m                      \u001b[1;34m'Found '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' input samples '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 744\u001b[1;33m                      'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[0;32m    745\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m     raise ValueError('All sample_weight arrays should have '\n",
      "\u001b[1;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 15602 input samples and 4373 target samples."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "embedding_size = 128\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Embedding(50000, embedding_size, input_shape= (X_train.shape[1],)))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.LSTM(units=128,  activation='sigmoid'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=4, batch_size = 128,)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24004, 50)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "np.shape(X_claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = X_metadata[indexw]\n",
    "yy = y_train_num[indexw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy\n",
    "\n",
    "yy= np.where(yy == 96, 1, 0)\n",
    "\n",
    "yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = ms.train_test_split(xx[50:], yy[50:], train_size=0.65,test_size=0.35, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (4340, 33)\n",
      "x_test shape: (2338, 33)\n",
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\einar\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4340 samples, validate on 2338 samples\n",
      "Epoch 1/3\n",
      "4340/4340 [==============================] - 1s 221us/step - loss: 0.5543 - accuracy: 0.7101 - val_loss: 0.5225 - val_accuracy: 0.7173\n",
      "Epoch 2/3\n",
      "4340/4340 [==============================] - 0s 110us/step - loss: 0.5064 - accuracy: 0.7459 - val_loss: 0.5108 - val_accuracy: 0.7335\n",
      "Epoch 3/3\n",
      "4340/4340 [==============================] - 0s 110us/step - loss: 0.4332 - accuracy: 0.8318 - val_loss: 0.5429 - val_accuracy: 0.7340\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2384fef50c8>"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# set parameters:\n",
    "max_features = 5000\n",
    "maxlen = 33\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 500\n",
    "epochs = 3\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p = model.predict_classes(xx[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "r= yy[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alle = len(r)\n",
    "rett =0\n",
    "for i,e in zip(p,r):\n",
    "    if i==e:\n",
    "        rett+=1\n",
    "        \n",
    "    \n",
    "    \n",
    "rett/alle   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
